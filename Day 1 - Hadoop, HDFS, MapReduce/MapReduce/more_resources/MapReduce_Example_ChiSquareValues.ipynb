{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38c0fb3",
   "metadata": {},
   "source": [
    "# Computing Chi Square Values to determine the importance of each word in a corpus\n",
    "\n",
    "### The corpus consists of multiple Amazon reviews belonging to different categories. https://jmcauley.ucsd.edu/data/amazon/ \n",
    "\n",
    "The aim is to first preprocess the dataset by splitting, tokenizing and filtering stop words (the stop word list is located under /lakobian/stopwords.txt but any stop word list works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a945f17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4f6a61558b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "import operator\n",
    "import re\n",
    "import pyspark.sql.functions as f\n",
    "from collections import defaultdict\n",
    "from pyspark.rdd import RDD\n",
    "import heapq\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "\n",
    "tf = sc.textFile(\"hdfs://c101.local:8020/user/lakobian/devset/reviews_devset.json\")\n",
    "sw = sc.textFile(\"hdfs://c101.local:8020/user/lakobian/stopwords.txt\")\n",
    "stopw = sw.flatMap(lambda x: x.split(\" \\n\"))\n",
    "stopwords = sc.broadcast(sw.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4886e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "507d8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    category = doc[doc.find('\\\"category\\\":')+13:doc.find('\"}')]\n",
    "    text = doc[doc.find('\\\"reviewText\\\":')+14:doc.find('\\\"overall\\\"')]\n",
    "    docs = re.split('[ \\t\\\"\\'`~#&*%$\\\\\\\\/0123456789.!?,;:()\\\\[\\\\]{}-]+',text)\n",
    "    docs = list(filter(None, docs))\n",
    "    docs = list(map(lambda word: word.lower(),docs))\n",
    "    docs = list(set(filter(lambda word: word not in stopwords.value and (len(word) > 1), docs)))\n",
    "    return (category,docs)\n",
    "\n",
    "## the first mapper and reducer count the number of reviews in each category, and the count of a word in the corpus\n",
    "def mapper_wordcount(row):\n",
    "    review_text = row[1]\n",
    "    category = row[0]\n",
    "    \n",
    "    document = []\n",
    "    catisIncremented = 0\n",
    "    if len(category)>1:\n",
    "        for word in review_text:\n",
    "            wordliste = []\n",
    "            wordliste.append(word)\n",
    "            wordliste.append(category)\n",
    "            if not catisIncremented:\n",
    "                wordliste.append(1)\n",
    "                catisIncremented = 1\n",
    "            else:\n",
    "                wordliste.append(0)\n",
    "\n",
    "            document.append(wordliste)\n",
    "        \n",
    "    return document\n",
    "\n",
    "\n",
    "def reduce_wordcount(a,b):\n",
    "    c = a.copy()\n",
    "    if b.keys() <= c.keys():\n",
    "        liste = c[list(b.keys())[0]]\n",
    "        liste[1] += 2\n",
    "        liste[0] += list(b.values())[0][0]\n",
    "        c[list(b.keys())[0]] = liste\n",
    "    else:\n",
    "        c.update(b)\n",
    "    return c\n",
    "\n",
    "\n",
    "def split(doc):\n",
    "    docs = re.split('[`~#&*%$\\\\](){}]+',doc)\n",
    "    docs = list(filter(None, docs))\n",
    "    overall_liste = []\n",
    "    #del(docs[:1])\n",
    "    word = docs[0].replace(\", \", \"\")\n",
    "    wordcount = 0\n",
    "    for doc in docs[1:]:\n",
    "        splits = re.split('[ \\t\\\"\\'`~#&*%$\\\\\\\\/.!?,;:()\\\\[\\\\]{}-]+',doc)\n",
    "        splits = list(filter(None, splits))\n",
    "        wordcount += int(splits[2])\n",
    "    for doc in docs[1:]:\n",
    "        liste = []\n",
    "        splits = re.split('[ \\t\\\"\\'`~#&*%$\\\\\\\\/.!?,;:()\\\\[\\\\]{}-]+',doc)\n",
    "        splits = list(filter(None, splits))\n",
    "        #word = splits[0]\n",
    "        liste.append(splits[0])\n",
    "        liste.append(splits[1])\n",
    "        liste.append(splits[2])\n",
    "        liste.append(wordcount)\n",
    "        liste.append(word)\n",
    "\n",
    "        overall_liste.append(liste)\n",
    "    return overall_liste\n",
    "\n",
    "def mapper_catcount(row):\n",
    "    \n",
    "    category = row[0]\n",
    "    info = row[1:]\n",
    "    word = info[-1]\n",
    "    liste = []\n",
    "    liste += info[:-1]\n",
    "    return (category,{word:liste})\n",
    "\n",
    "def reduce_catcount(a,b):\n",
    "    c = a.copy()\n",
    "    if b.keys() <= c.keys():\n",
    "        raise RuntimeError from exc\n",
    "    else:\n",
    "        c.update(b)\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c06bd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9ac20f6ab504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessed_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessed_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessed_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmapper_wordcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreduce_wordcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessed_texts = tf.map(lambda doc: preprocess(doc))\n",
    "N = preprocessed_texts.count()\n",
    "mapped = preprocessed_texts.flatMap(lambda row: mapper_wordcount(row)).map(lambda row: (row[0],{row[1]:[row[2],1]}))\n",
    "reduced = mapped.reduceByKey(lambda a, b: reduce_wordcount(a,b))\n",
    "\n",
    "##Output directories need to be newly created! Change the name!\n",
    "reduced.coalesce(1).saveAsTextFile(\"hdfs://c101.local:8020/user/lakobian/res1\")\n",
    "\n",
    "temp_input = sc.textFile(\"hdfs://c101.local:8020/user/lakobian/res1\")\n",
    "texts = temp_input.map(lambda doc: doc)\n",
    "\n",
    "split_texts = texts.flatMap(lambda doc: split(doc)).map(lambda row: mapper_catcount(row))\n",
    "reduced_texts = split_texts.reduceByKey(lambda a,b: reduce_catcount(a,b))\n",
    "reduced_texts.coalesce(1).saveAsTextFile(\"hdfs://c101.local:8020/user/lakobian/res2\")\n",
    "temp_input2 = sc.textFile(\"hdfs://c101.local:8020/user/lakobian/res2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c0009",
   "metadata": {},
   "source": [
    "## Chi Square Computation \n",
    "as per http://www.ccs.neu.edu/home/vip/teach/DMcourse/3_dim_reduction/notes_slides/ChiSquare_FeatureSelection.pdf \n",
    "\n",
    "### For each category all words are sorted by their chi square value in descending order && top 150 words of each category are stored in a merged dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5df0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(doc):\n",
    "    docs = re.split('[`~#&*%$\\\\](){}]+',doc)\n",
    "    docs = list(filter(None, docs))\n",
    "    overall_liste = []\n",
    "    docs[0] = docs[0].replace(\", \", \"\")\n",
    "    P = 0\n",
    "    chivalue = 0.0\n",
    "    for doc in docs[1:]:\n",
    "        split = re.split('[ \\t\\\"\\'`~#&*%$\\\\\\\\/.!?,;:()\\\\[\\\\]{}-]+',doc)\n",
    "        split = list(filter(None, split))\n",
    "        P += int(split[1])\n",
    "    for doc in docs[1:]:\n",
    "        liste = []\n",
    "        splits = re.split('[ \\t\\\"\\'`~#&*%$\\\\\\\\/.!?,;:()\\\\[\\\\]{}-]+',doc)\n",
    "        splits = list(filter(None, splits))\n",
    "        #word = splits[0]\n",
    "        A = float(splits[2])\n",
    "        M = float(splits[3])\n",
    "        B = (M - A)\n",
    "        C = (P - A)\n",
    "        D = (N - (A + B + C))\n",
    "        try:\n",
    "            chivalue = (float(N)*(abs((A*D)-(B*C))**2))//((A + C)*(B + D)*(A + B)*(C + D))\n",
    "        except ZeroDivisionError:\n",
    "            print(0)\n",
    "\n",
    "        overall_liste.append((docs[0],[splits[0],chivalue]))\n",
    "    return (overall_liste)\n",
    "\n",
    "\n",
    "def orderByKey(self, num, sortValue = None):\n",
    " \n",
    "        def init(a):\n",
    "            return [a]\n",
    " \n",
    "        def combine(agg, a):\n",
    "            agg.append(a)\n",
    "            return getTopN(agg)\n",
    " \n",
    "        def merge(a, b):\n",
    "            agg = a + b\n",
    "            return getTopN(agg)\n",
    " \n",
    "        def getTopN(agg):\n",
    "            return heapq.nlargest(num, agg, sortValue)            \n",
    " \n",
    "        return self.combineByKey(init, combine, merge)\n",
    "\n",
    "    \n",
    "RDD.orderByKey = orderByKey\n",
    "top_entries = temp_input2.flatMap(lambda doc: split(doc)).orderByKey(150, sortValue=lambda x: x[1]).coalesce(1)\n",
    "merged_dictionary = top_entries.flatMap(lambda a: a[1]).sortBy(lambda a: -a[1]).coalesce(1).map(lambda a: (\"merged dictionary\",a)).reduceByKey(lambda a,b:a+b)\n",
    "both = top_entries.union(merged_dictionary)\n",
    "both.saveAsTextFile(\"hdfs://c101.local:8020/user/lakobian/final_res\")\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765ae4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3a1a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jpovh/Rscripts/big-data-training/code/Python'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3952bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
