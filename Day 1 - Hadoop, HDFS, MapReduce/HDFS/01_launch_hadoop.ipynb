{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc150195-e209-45ed-8558-e024c3186bd1",
   "metadata": {},
   "source": [
    "# Launch a Hadoop cluster on the VSC-4 cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e1168-b9db-440c-ac72-f29376ea61ed",
   "metadata": {},
   "source": [
    "## Create a batch script\n",
    "\n",
    "This is a SLURM script that is going to run on one or more VSC-4 nodes.\n",
    "\n",
    "The only command that is going to run on the VSC-4 cluster once Hadoop has started is: `echo 'Hello, World!'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3025c429-170c-4e11-9bcb-2dd2802b7dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing launch_Hadoop_cluster.slrm\n"
     ]
    }
   ],
   "source": [
    "%%writefile launch_Hadoop_cluster.slrm\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=launch_Hadoop_cluster  # name of the job\n",
    "#SBATCH --nodes=1                         # number of nodes reserved\n",
    "#SBATCH --time=00:04:00                   # time needed for job\n",
    "#SBATCH --partition=skylake_0096  \n",
    "#SBATCH --qos=skylake_0096\n",
    "#SBATCH --reservation=training            # available only during the course\n",
    "\n",
    "\n",
    "###################################################\n",
    "# Use SBATCH --reservation=training during training\n",
    "\n",
    "\n",
    "##########################################\n",
    "# clear all modules & load Java and Hadoop\n",
    "module purge\n",
    "module load openjdk\n",
    "module load hadoop\n",
    "\n",
    "################################\n",
    "# set some environment variables\n",
    "export PDSH_RCMD_TYPE=ssh\n",
    "\n",
    "#######################\n",
    "# launch Hadoop cluster\n",
    "prolog_create_key.sh\n",
    "source vsc_start_hadoop.sh\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# EDIT BEGIN\n",
    "# here the cluster is running and we can run our commands\n",
    "# echo 'Hello, World!'\n",
    "echo 'Hello, World!'\n",
    "# EDIT END\n",
    "#########################################################\n",
    "\n",
    "\n",
    "#####################\n",
    "# stop Hadoop cluster\n",
    "source vsc_stop_hadoop.sh\n",
    "epilog_discard_key.sh\n",
    "\n",
    "# check output/errors in slurm-<jobID>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c87003-ec63-4ccf-b21c-4d880bc7c097",
   "metadata": {},
   "source": [
    "## Launch batch script\n",
    "\n",
    "Launch `launch_Hadoop_cluster.slrm` script to start a Hadoop cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3744fffa-939a-4a62-b0a4-ad2fe4bf3efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 38921\n"
     ]
    }
   ],
   "source": [
    "!sbatch launch_Hadoop_cluster.slrm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eecc43-4260-430e-8fdb-bdaae5de2a65",
   "metadata": {},
   "source": [
    "### Check all your SLURM jobs.\n",
    "\n",
    "With `slurm -u $USER` you can see all your SLURM jobs (queued or running).\n",
    "\n",
    "**Note:** The job running in partition `jupyter` is the job responsible for delivering the current Jupyter Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc62c78e-a5bd-48f8-9192-0feec5c2952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   JOBID PARTITION            NAME      USER    STATE     TIME TIME_LIMIT NODES NODELIST(REASON)\n",
      "   38529 skylake_0 vsc4_jh_conda_t trainee20  RUNNING  1:39:38   12:00:00     1 n4901-003\n",
      "   38921 skylake_0 launch_Hadoop_c trainee20  RUNNING  INVALID       4:00     1 n4901-018\n"
     ]
    }
   ],
   "source": [
    "!squeue --format='%.8i %.9P %.15j %.9u %.8T %.8M %.10l %.5D %R' --me -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc19676-94f8-4d08-9203-ed1a29879431",
   "metadata": {},
   "source": [
    "Check queue again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed42431-80b4-4c66-b176-58d7d0fdb418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   JOBID PARTITION            NAME      USER    STATE     TIME TIME_LIMIT NODES NODELIST(REASON)\n",
      "   38529 skylake_0 vsc4_jh_conda_t trainee20  RUNNING  1:39:51   12:00:00     1 n4901-003\n",
      "   38921 skylake_0 launch_Hadoop_c trainee20  RUNNING  INVALID       4:00     1 n4901-018\n"
     ]
    }
   ],
   "source": [
    "!squeue --format='%.8i %.9P %.15j %.9u %.8T %.8M %.10l %.5D %R' --me -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9ba70-4ff4-48a9-8649-a5542e64c583",
   "metadata": {},
   "source": [
    "### Launch job keeping track of job ID\n",
    "\n",
    "We're going to launch the same script again but this time we keep track of the job number so we don't need to type it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982c41aa-65d4-483a-accc-12cf1ac39d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_output = !sbatch launch_Hadoop_cluster.slrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76847720-223c-4ec2-b8f7-d7e178d429e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job number = 38923\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "JOBNR=bash_output[0].split()[-1]\n",
    "print(\"Job number = {}\".format(JOBNR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ba5bc-9fdb-4698-8434-41daa76a63ea",
   "metadata": {},
   "source": [
    "#### Check the queue\n",
    "\n",
    "You can re-run the following cell until job is running or completed.\n",
    "\n",
    "**Note:** Use `ctrl-enter` to run a cell keeping the cursor in the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0dff27b-9552-4d46-8ae0-9c42ac0eaf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   JOBID PARTITION            NAME      USER    STATE     TIME TIME_LIMIT NODES NODELIST(REASON)\n",
      "   38923 skylake_0 launch_Hadoop_c trainee20  RUNNING  INVALID       4:00     1 n4901-020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"squeue --format='%.8i %.9P %.15j %.9u %.8T %.8M %.10l %.5D %R' --me -j {}\".format(JOBNR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f7217-dbaf-4c55-9fed-e444c254a6f6",
   "metadata": {},
   "source": [
    "### Check other participants' jobs\n",
    "\n",
    "Check jobs from other participants in the queue by filtering for user named `trainee*` in the SLURM queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acdea31f-7e91-4a0d-9504-1cdf0b416c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38921 skylake_0 launch_Hadoop_c trainee20  RUNNING     0:05       4:00     1 n4901-018\n",
      "   38923 skylake_0 launch_Hadoop_c trainee20  RUNNING  INVALID       4:00     1 n4901-020\n",
      "   38922 skylake_0 launch_Hadoop_c trainee32  RUNNING  INVALID       4:00     1 n4901-019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"squeue -R'training' --format='%.8i %.9P %.15j %.9u %.8T %.8M %.10l %.5D %R'|grep trainee|sort -k3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89543d9f-a506-4971-9967-201d8800d7c4",
   "metadata": {},
   "source": [
    "Check queue again to see progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c890ed7-303b-449d-8e6b-628f84bbb075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38921 skylake_0 launch_Hadoop_c trainee20  RUNNING     0:07       4:00     1 n4901-018\n",
      "   38923 skylake_0 launch_Hadoop_c trainee20  RUNNING  INVALID       4:00     1 n4901-020\n",
      "   38922 skylake_0 launch_Hadoop_c trainee32  RUNNING  INVALID       4:00     1 n4901-019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"squeue -R'training' --format='%.8i %.9P %.15j %.9u %.8T %.8M %.10l %.5D %R'|grep trainee|sort -k3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24140edd-4ad4-4fe1-ae75-c871a0948d37",
   "metadata": {},
   "source": [
    "### View results\n",
    "When the job does not appear in the queue anymore, you can find the output in a file called `slurm-<JOBNR>.out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c9ceed-08bb-4ffc-ac88-21644a479cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 trainee20 p70824 54077 Oct 19 14:59 slurm-38923.out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"ls -l slurm-{}.out\".format(JOBNR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7afb09-b136-401c-836d-fbd25aa3e72b",
   "metadata": {},
   "source": [
    "## Check SLURM job run time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705956c7-8096-420f-9fa9-75794af31d3e",
   "metadata": {},
   "source": [
    "### With `scontrol`\n",
    "\n",
    "While the job is running or shortly after its completion, one can get information about it with `scontrol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94bb3e8-7f09-44ca-a752-51de46fd9b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobId=38923 JobName=launch_Hadoop_cluster\n",
      "   UserId=trainee20(73846) GroupId=p70824(70824) MCS_label=N/A\n",
      "   Priority=958 Nice=0 Account=p70824 QOS=skylake_0096\n",
      "   JobState=RUNNING Reason=None Dependency=(null)\n",
      "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
      "   RunTime=INVALID TimeLimit=00:04:00 TimeMin=N/A\n",
      "   SubmitTime=2022-10-19T14:59:46 EligibleTime=2022-10-19T14:59:46\n",
      "   AccrueTime=2022-10-19T14:59:46\n",
      "   StartTime=2022-10-19T14:59:48 EndTime=2022-10-19T15:03:48 Deadline=N/A\n",
      "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2022-10-19T14:59:48 Scheduler=Main\n",
      "   Partition=skylake_0096 AllocNode:Sid=n4901-003:2624075\n",
      "   ReqNodeList=(null) ExcNodeList=(null)\n",
      "   NodeList=n4901-020\n",
      "   BatchHost=n4901-020\n",
      "   NumNodes=1 NumCPUs=96 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n",
      "   TRES=cpu=96,mem=96296M,node=1,billing=96\n",
      "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:1 CoreSpec=*\n",
      "   MinCPUsNode=1 MinMemoryNode=0 MinTmpDiskNode=0\n",
      "   Features=(null) DelayBoot=00:00:00\n",
      "   Reservation=training\n",
      "   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)\n",
      "   Command=/home/fs70824/trainee20/hadoop_training/HDFS/launch_Hadoop_cluster.slrm\n",
      "   WorkDir=/home/fs70824/trainee20/hadoop_training/HDFS\n",
      "   StdErr=/home/fs70824/trainee20/hadoop_training/HDFS/slurm-38923.out\n",
      "   StdIn=/dev/null\n",
      "   StdOut=/home/fs70824/trainee20/hadoop_training/HDFS/slurm-38923.out\n",
      "   Power=\n",
      "   TresPerNode=gres:cpu_skylake_0096:32\n",
      "   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"scontrol show job {}\".format(JOBNR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85beea-28ab-4573-8776-da93b01aa558",
   "metadata": {},
   "source": [
    "Or equivalently, using a bash command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3422499-a241-4423-8db5-9f0b47fd3521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobId=38923 JobName=launch_Hadoop_cluster\n",
      "   UserId=trainee20(73846) GroupId=p70824(70824) MCS_label=N/A\n",
      "   Priority=958 Nice=0 Account=p70824 QOS=skylake_0096\n",
      "   JobState=RUNNING Reason=None Dependency=(null)\n",
      "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
      "   RunTime=INVALID TimeLimit=00:04:00 TimeMin=N/A\n",
      "   SubmitTime=2022-10-19T14:59:46 EligibleTime=2022-10-19T14:59:46\n",
      "   AccrueTime=2022-10-19T14:59:46\n",
      "   StartTime=2022-10-19T14:59:48 EndTime=2022-10-19T15:03:48 Deadline=N/A\n",
      "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2022-10-19T14:59:48 Scheduler=Main\n",
      "   Partition=skylake_0096 AllocNode:Sid=n4901-003:2624075\n",
      "   ReqNodeList=(null) ExcNodeList=(null)\n",
      "   NodeList=n4901-020\n",
      "   BatchHost=n4901-020\n",
      "   NumNodes=1 NumCPUs=96 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n",
      "   TRES=cpu=96,mem=96296M,node=1,billing=96\n",
      "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:1 CoreSpec=*\n",
      "   MinCPUsNode=1 MinMemoryNode=0 MinTmpDiskNode=0\n",
      "   Features=(null) DelayBoot=00:00:00\n",
      "   Reservation=training\n",
      "   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)\n",
      "   Command=/home/fs70824/trainee20/hadoop_training/HDFS/launch_Hadoop_cluster.slrm\n",
      "   WorkDir=/home/fs70824/trainee20/hadoop_training/HDFS\n",
      "   StdErr=/home/fs70824/trainee20/hadoop_training/HDFS/slurm-38923.out\n",
      "   StdIn=/dev/null\n",
      "   StdOut=/home/fs70824/trainee20/hadoop_training/HDFS/slurm-38923.out\n",
      "   Power=\n",
      "   TresPerNode=gres:cpu_skylake_0096:32\n",
      "   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!scontrol show job {JOBNR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927c3539-ee34-4916-8fce-ee516a67abf4",
   "metadata": {},
   "source": [
    "### With `sacct`\n",
    "\n",
    "After completion, information on the job can be retrieved with `sacct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b681e219-a6fc-4cde-8ee1-d82942cb8564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobID           JobName     MaxRSS    Elapsed \n",
      "------------ ---------- ---------- ---------- \n",
      "38923        launch_Ha+              00:00:18 \n",
      "38923.batch       batch              00:00:18 \n",
      "38923.extern     extern              00:00:18 \n"
     ]
    }
   ],
   "source": [
    "!sacct -j {JOBNR} --format=JobID,JobName,MaxRSS,Elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba038f-2b3f-4181-bca8-6efb5f89921b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
