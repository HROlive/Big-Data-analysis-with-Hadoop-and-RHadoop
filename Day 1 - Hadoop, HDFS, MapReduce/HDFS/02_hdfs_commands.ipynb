{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc150195-e209-45ed-8558-e024c3186bd1",
   "metadata": {},
   "source": [
    "# Run some HDFS commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546f05c-576a-4c40-9d2e-ae9cde73748b",
   "metadata": {},
   "source": [
    "First, we need to launch a Hadoop cluster on the VSC-4 cluster.\n",
    "\n",
    "After starting the cluster, we can run our commands.\n",
    "\n",
    "Finally, we can shutdown the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea89d09-f3f1-47b0-b5ef-37ddc6c34657",
   "metadata": {},
   "source": [
    "### Edit SLURM script\n",
    "\n",
    "The only part that needs to be edited is from `EDIT BEGIN` to `EDIT END`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3025c429-170c-4e11-9bcb-2dd2802b7dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hdfs_commands.slrm\n"
     ]
    }
   ],
   "source": [
    "%%writefile hdfs_commands.slrm\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=hdfs_commands      # name of the job\n",
    "#SBATCH --nodes=2                     # number of nodes reserved\n",
    "#SBATCH --time=00:04:00               # time needed for job\n",
    "#SBATCH --partition=skylake_0096  \n",
    "#SBATCH --qos=skylake_0096\n",
    "#SBATCH --reservation=training        # available only during the course\n",
    "\n",
    "\n",
    "###################################################\n",
    "# Use SBATCH --reservation=training during training\n",
    "\n",
    "##########################################\n",
    "# clear all modules & load Java and Hadoop\n",
    "module purge\n",
    "module load openjdk\n",
    "module load hadoop\n",
    "\n",
    "################################\n",
    "# set some environment variables\n",
    "export PDSH_RCMD_TYPE=ssh\n",
    "\n",
    "#######################\n",
    "# launch Hadoop cluster\n",
    "prolog_create_key.sh\n",
    "source vsc_start_hadoop.sh\n",
    "\n",
    "\n",
    "#########################################################\n",
    "# EDIT BEGIN\n",
    "# here the cluster is running and we can run our commands\n",
    "\n",
    "DATADIR='/home/fs70824/training/hadoop_training_data'   # this is where data is located \n",
    "\n",
    "echo \"Creating a new directory myDir\"\n",
    "hdfs dfs -mkdir myDir\n",
    "\n",
    "echo \"List new HDFS directory myDir\"\n",
    "hdfs dfs -ls -h myDir \n",
    "\n",
    "echo \"Upload file to new directory myDir\"\n",
    "hdfs dfs -put ${DATADIR}/wiki_sample_2400lines myDir\n",
    "\n",
    "echo \"List new directory myDir\"\n",
    "hdfs dfs -ls -h myDir\n",
    "\n",
    "echo \"Show disk usage of directory myDir\"\n",
    "hdfs dfs -du -h myDir \n",
    "\n",
    "# change replication factor to 2\n",
    "hdfs dfs -setrep -w 2 myDir\n",
    "\n",
    "echo \"Show disk usage of directory myDir after changing replication factor\"\n",
    "hdfs dfs -du -h myDir \n",
    "\n",
    "echo \"Remove directory myDir (this will remove also all files contained in it\"\n",
    "hdfs dfs -rm -r myDir\n",
    "\n",
    "# EDIT END\n",
    "#########################################################\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "# stop Hadoop cluster\n",
    "source vsc_stop_hadoop.sh\n",
    "epilog_discard_key.sh\n",
    "\n",
    "# check output/errors in slurm-<jobID>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c87003-ec63-4ccf-b21c-4d880bc7c097",
   "metadata": {},
   "source": [
    "### Launch batch script\n",
    "\n",
    "Launch `hdfs_commands.slrm` script to start a Hadoop cluster and run some HDFS commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9ba70-4ff4-48a9-8649-a5542e64c583",
   "metadata": {},
   "source": [
    "### Launch batch script keeping track of job ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982c41aa-65d4-483a-accc-12cf1ac39d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_output = !sbatch hdfs_commands.slrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76847720-223c-4ec2-b8f7-d7e178d429e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job number = 38946\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "JOBNR=bash_output[0].split()[-1]\n",
    "print(\"Job number = {}\".format(JOBNR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ba5bc-9fdb-4698-8434-41daa76a63ea",
   "metadata": {},
   "source": [
    "#### Check the queue\n",
    "\n",
    "You can re-run the following cell until job is running or completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0dff27b-9552-4d46-8ae0-9c42ac0eaf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   JOBID PARTITION            NAME      USER    STATE     TIME TIME_LIMIT NODES NODELIST(REASON)\n",
      "   38946 skylake_0   hdfs_commands trainee20  PENDING     0:00       4:00     2 (Resources)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"squeue --format='%.8i %.9P %.15j %.9u %.8T %.8M %.10l %.5D %R' --me -j {}\".format(JOBNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58dcde27-1fa8-4796-9cee-7afed4b8cc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   JOBID PARTITION            NAME      USER    STATE     TIME TIME_LIMIT NODES NODELIST(REASON)\n",
      "   38946 skylake_0   hdfs_commands trainee20  RUNNING  INVALID       4:00     2 n4901-[015-016]\n",
      "   38529 skylake_0 vsc4_jh_conda_t trainee20  RUNNING  1:47:53   12:00:00     1 n4901-003\n"
     ]
    }
   ],
   "source": [
    "!squeue --format='%.8i %.9P %.15j %.9u %.8T %.8M %.10l %.5D %R' --me -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1bf3d-b08e-4d47-8b6e-f8dfeca3f06c",
   "metadata": {},
   "source": [
    "### Check other participants' jobs\n",
    "\n",
    "Check jobs from other participants in the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c3b3727-3afa-4904-b961-a62d772b4cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38941 skylake_0   hdfs_commands trainee01  RUNNING     0:31       4:00     2 n4901-[022-023]\n",
      "   38947 skylake_0   hdfs_commands trainee08  RUNNING  INVALID       4:00     2 n4901-[020-021]\n",
      "   38945 skylake_0   hdfs_commands trainee15  RUNNING  INVALID       4:00     2 n4901-[009-010]\n",
      "   38946 skylake_0   hdfs_commands trainee20  RUNNING  INVALID       4:00     2 n4901-[015-016]\n",
      "   38940 skylake_0   hdfs_commands trainee32  RUNNING     0:38       4:00     2 n4901-[018-019]\n",
      "   38942 skylake_0   hdfs_commands trainee35  RUNNING     0:25       4:00     2 n4901-[011-012]\n",
      "   38944 skylake_0   hdfs_commands trainee35  RUNNING  INVALID       4:00     2 n4901-[013-014]\n",
      "   38950 skylake_0   hdfs_commands trainee97  PENDING     0:00       4:00     2 (Resources)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"squeue -R'training' --format='%.8i %.9P %.15j %.9u %.8T %.8M %.10l %.5D %R'|grep trainee|sort -k3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f0293c-0a36-494c-bdb8-c6b111461cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38947 skylake_0   hdfs_commands trainee08  RUNNING  INVALID       4:00     2 n4901-[020-021]\n",
      "   38945 skylake_0   hdfs_commands trainee15  RUNNING     0:17       4:00     2 n4901-[009-010]\n",
      "   38946 skylake_0   hdfs_commands trainee20  RUNNING  INVALID       4:00     2 n4901-[015-016]\n",
      "   38944 skylake_0   hdfs_commands trainee35  RUNNING     0:08       4:00     2 n4901-[013-014]\n",
      "   38950 skylake_0   hdfs_commands trainee97  RUNNING  INVALID       4:00     2 n4901-[018-019]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"squeue -R'training' --format='%.8i %.9P %.15j %.9u %.8T %.8M %.10l %.5D %R'|grep trainee|sort -k3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24140edd-4ad4-4fe1-ae75-c871a0948d37",
   "metadata": {},
   "source": [
    "#### Check results\n",
    "When the job does not appear in the queue anymore, you can check the output in a file called `slurm-<JOBNR>.out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c9ceed-08bb-4ffc-ac88-21644a479cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 trainee20 p70824 54996 Oct 19 15:07 slurm-38946.out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"ls -l slurm-{}.out\".format(JOBNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a302f0-620d-4adb-8703-c44b5c92e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobId=38946 JobName=hdfs_commands\n",
      "   UserId=trainee20(73846) GroupId=p70824(70824) MCS_label=N/A\n",
      "   Priority=961 Nice=0 Account=p70824 QOS=skylake_0096\n",
      "   JobState=RUNNING Reason=None Dependency=(null)\n",
      "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
      "   RunTime=00:00:39 TimeLimit=00:04:00 TimeMin=N/A\n",
      "   SubmitTime=2022-10-19T15:06:59 EligibleTime=2022-10-19T15:06:59\n",
      "   AccrueTime=2022-10-19T15:06:59\n",
      "   StartTime=2022-10-19T15:07:23 EndTime=2022-10-19T15:11:23 Deadline=N/A\n",
      "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2022-10-19T15:07:23 Scheduler=Main\n",
      "   Partition=skylake_0096 AllocNode:Sid=n4901-003:2624425\n",
      "   ReqNodeList=(null) ExcNodeList=(null)\n",
      "   NodeList=n4901-[015-016]\n",
      "   BatchHost=n4901-015\n",
      "   NumNodes=2 NumCPUs=192 NumTasks=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n",
      "   TRES=cpu=192,mem=192592M,node=2,billing=192\n",
      "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:1 CoreSpec=*\n",
      "   MinCPUsNode=1 MinMemoryNode=0 MinTmpDiskNode=0\n",
      "   Features=(null) DelayBoot=00:00:00\n",
      "   Reservation=training\n",
      "   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)\n",
      "   Command=/home/fs70824/trainee20/hadoop_training/HDFS/hdfs_commands.slrm\n",
      "   WorkDir=/home/fs70824/trainee20/hadoop_training/HDFS\n",
      "   StdErr=/home/fs70824/trainee20/hadoop_training/HDFS/slurm-38946.out\n",
      "   StdIn=/dev/null\n",
      "   StdOut=/home/fs70824/trainee20/hadoop_training/HDFS/slurm-38946.out\n",
      "   Power=\n",
      "   TresPerNode=gres:cpu_skylake_0096:32\n",
      "   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!scontrol show job {JOBNR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84559a36-eab9-4b5b-a63a-56ff959e4972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobID           JobName     MaxRSS    Elapsed \n",
      "------------ ---------- ---------- ---------- \n",
      "38946        hdfs_comm+              00:01:27 \n",
      "38946.batch       batch    817476K   00:01:27 \n",
      "38946.extern     extern          0   00:01:28 \n"
     ]
    }
   ],
   "source": [
    "!sacct -j {JOBNR} --format=JobID,JobName,MaxRSS,Elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73827aeb-b90a-4aae-ad7a-b4bed6269487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
